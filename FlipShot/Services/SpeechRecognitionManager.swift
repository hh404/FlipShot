//
//  SpeechRecognitionManager.swift
//  FlipShot
//
//  è¯­éŸ³è¯†åˆ«ç®¡ç†å™¨ï¼šè¯†åˆ«"ä¸Š/ä¸‹/å·¦/å³"æ–¹å‘æŒ‡ä»¤
//

import Speech
import AVFoundation

final class SpeechRecognitionManager: NSObject {
    
    static let shared = SpeechRecognitionManager()
    
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    /// è¯†åˆ«åˆ°æ–¹å‘çš„å›è°ƒ
    var onDirectionRecognized: ((EDirection) -> Void)?
    
    /// è¯†åˆ«åˆ°"ä¸‹ä¸€ä¸ª"æŒ‡ä»¤çš„å›è°ƒ
    var onNextCommand: (() -> Void)?
    
    /// è¯†åˆ«çŠ¶æ€å˜åŒ–å›è°ƒ
    var onStatusChanged: ((Bool) -> Void)?  // true = æ­£åœ¨è¯†åˆ«, false = å·²åœæ­¢
    
    /// ä¸Šæ¬¡è¯†åˆ«åˆ°çš„æ–¹å‘ï¼ˆç”¨äºé˜²æ­¢é‡å¤è§¦å‘ï¼‰
    private var lastRecognizedDirection: EDirection?
    private var lastRecognizedTime: Date?
    
    /// ä¸Šæ¬¡è¯†åˆ«åˆ°"ä¸‹ä¸€ä¸ª"æŒ‡ä»¤çš„æ—¶é—´ï¼ˆç”¨äºé˜²æ­¢é‡å¤è§¦å‘ï¼‰
    private var lastNextCommandTime: Date?
    
    /// ä¸Šæ¬¡å¤„ç†çš„æ–‡æœ¬é•¿åº¦ï¼ˆç”¨äºåªå¤„ç†æ–°å¢éƒ¨åˆ†ï¼‰
    private var lastProcessedLength: Int = 0
    
    /// è¿ç»­"æ— è¯­éŸ³"é”™è¯¯è®¡æ•°
    private var consecutiveNoSpeechErrors: Int = 0
    private let maxConsecutiveErrors = 3
    
    private override init() {
        super.init()
    }
    
    /// è¯·æ±‚è¯­éŸ³è¯†åˆ«æƒé™
    func requestAuthorization(completion: @escaping (Bool) -> Void) {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    completion(true)
                case .denied, .restricted, .notDetermined:
                    completion(false)
                @unknown default:
                    completion(false)
                }
            }
        }
    }
    
    /// å¼€å§‹è¯­éŸ³è¯†åˆ«
    func startRecognition() throws {
        print("ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ«...")
        
        // æ£€æµ‹æ˜¯å¦åœ¨æ¨¡æ‹Ÿå™¨ä¸Šè¿è¡Œ
        #if targetEnvironment(simulator)
        print("âš ï¸ æ£€æµ‹åˆ°æ¨¡æ‹Ÿå™¨ç¯å¢ƒï¼Œè¯­éŸ³è¯†åˆ«å¯èƒ½ä¸ç¨³å®š")
        print("âš ï¸ å»ºè®®ä½¿ç”¨çœŸæœºæµ‹è¯•è¯­éŸ³åŠŸèƒ½")
        // æ¨¡æ‹Ÿå™¨ç¯å¢ƒä¸‹ï¼Œä»ç„¶å°è¯•å¯åŠ¨ï¼Œä½†å¦‚æœå¤±è´¥ä¸æŠ›å‡ºé”™è¯¯
        #endif
        
        // å…ˆå®Œå…¨æ¸…ç†éŸ³é¢‘å¼•æ“
        if audioEngine.isRunning {
            print("âš ï¸ éŸ³é¢‘å¼•æ“å·²åœ¨è¿è¡Œï¼Œå…ˆåœæ­¢")
            audioEngine.stop()
        }
        
        // ç§»é™¤æ‰€æœ‰å·²å­˜åœ¨çš„ tapï¼ˆå¿½ç•¥é”™è¯¯ï¼‰
        let inputNode = audioEngine.inputNode
        do {
            inputNode.removeTap(onBus: 0)
            print("ğŸ§¹ ç§»é™¤æ—§çš„ tap")
        } catch {
            print("âš ï¸ ç§»é™¤ tap æ—¶å‡ºé”™ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰: \(error.localizedDescription)")
        }
        
        // å–æ¶ˆæ—§çš„è¯†åˆ«ä»»åŠ¡
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil
        
        // æ£€æŸ¥æƒé™
        let authStatus = SFSpeechRecognizer.authorizationStatus()
        print("ğŸ” è¯­éŸ³è¯†åˆ«æƒé™çŠ¶æ€: \(authStatus.rawValue)")
        guard authStatus == .authorized else {
            throw NSError(domain: "SpeechRecognition", code: 1, userInfo: [NSLocalizedDescriptionKey: "è¯­éŸ³è¯†åˆ«æœªæˆæƒ"])
        }
        
        // æ£€æŸ¥è¯†åˆ«å™¨æ˜¯å¦å¯ç”¨
        guard let recognizer = speechRecognizer, recognizer.isAvailable else {
            print("âŒ è¯­éŸ³è¯†åˆ«å™¨ä¸å¯ç”¨")
            throw NSError(domain: "SpeechRecognition", code: 3, userInfo: [NSLocalizedDescriptionKey: "è¯­éŸ³è¯†åˆ«å™¨ä¸å¯ç”¨"])
        }
        print("âœ… è¯­éŸ³è¯†åˆ«å™¨å¯ç”¨")
        
        // é…ç½®éŸ³é¢‘ä¼šè¯ï¼ˆå…è®¸æ’­æ”¾å’Œå½•éŸ³åŒæ—¶è¿›è¡Œï¼‰
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.playAndRecord, mode: .measurement, options: [.defaultToSpeaker, .allowBluetooth, .mixWithOthers])
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            print("âœ… éŸ³é¢‘ä¼šè¯é…ç½®æˆåŠŸ")
            
            // æ£€æŸ¥éº¦å…‹é£æƒé™
            switch audioSession.recordPermission {
            case .granted:
                print("âœ… éº¦å…‹é£æƒé™å·²æˆäºˆ")
            case .denied:
                print("âŒ éº¦å…‹é£æƒé™è¢«æ‹’ç»")
                throw NSError(domain: "SpeechRecognition", code: 6, userInfo: [
                    NSLocalizedDescriptionKey: "éº¦å…‹é£æƒé™è¢«æ‹’ç»ï¼Œè¯·åœ¨è®¾ç½®ä¸­å…è®¸"
                ])
            case .undetermined:
                print("âš ï¸ éº¦å…‹é£æƒé™æœªç¡®å®šï¼Œæ­£åœ¨è¯·æ±‚...")
                // æƒé™ä¼šåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨è¯·æ±‚
            @unknown default:
                print("âš ï¸ æœªçŸ¥çš„éº¦å…‹é£æƒé™çŠ¶æ€")
            }
        } catch {
            print("âŒ éŸ³é¢‘ä¼šè¯é…ç½®å¤±è´¥: \(error.localizedDescription)")
            throw error
        }
        
        // å…ˆå¯åŠ¨éŸ³é¢‘å¼•æ“ï¼ˆç¡®ä¿è¾“å…¥èŠ‚ç‚¹æ ¼å¼æœ‰æ•ˆï¼‰
        do {
            audioEngine.prepare()
            print("â³ éŸ³é¢‘å¼•æ“å‡†å¤‡ä¸­...")
            
            // è®¾ç½®è¶…æ—¶ä¿æŠ¤ï¼ˆæ¨¡æ‹Ÿå™¨å¯èƒ½å¡æ­»ï¼‰
            var engineStarted = false
            let startGroup = DispatchGroup()
            startGroup.enter()
            
            DispatchQueue.global(qos: .userInitiated).async {
                do {
                    try self.audioEngine.start()
                    engineStarted = true
                    print("âœ… éŸ³é¢‘å¼•æ“å·²å¯åŠ¨")
                } catch {
                    print("âŒ éŸ³é¢‘å¼•æ“å¯åŠ¨å¤±è´¥: \(error.localizedDescription)")
                }
                startGroup.leave()
            }
            
            // ç­‰å¾…æœ€å¤š 3 ç§’
            let timeout = startGroup.wait(timeout: .now() + 3.0)
            
            if timeout == .timedOut {
                print("âŒ éŸ³é¢‘å¼•æ“å¯åŠ¨è¶…æ—¶ï¼ˆå¯èƒ½æ˜¯æ¨¡æ‹Ÿå™¨é—®é¢˜ï¼‰")
                #if targetEnvironment(simulator)
                print("ğŸ’¡ è¯·åœ¨çœŸæœºä¸Šæµ‹è¯•è¯­éŸ³è¯†åˆ«åŠŸèƒ½")
                throw NSError(domain: "SpeechRecognition", code: 5, userInfo: [
                    NSLocalizedDescriptionKey: "æ¨¡æ‹Ÿå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«ï¼Œè¯·ä½¿ç”¨çœŸæœºæµ‹è¯•"
                ])
                #else
                throw NSError(domain: "SpeechRecognition", code: 5, userInfo: [
                    NSLocalizedDescriptionKey: "éŸ³é¢‘å¼•æ“å¯åŠ¨è¶…æ—¶"
                ])
                #endif
            }
            
            guard engineStarted else {
                throw NSError(domain: "SpeechRecognition", code: 5, userInfo: [
                    NSLocalizedDescriptionKey: "éŸ³é¢‘å¼•æ“å¯åŠ¨å¤±è´¥"
                ])
            }
        } catch {
            print("âŒ å¯åŠ¨éŸ³é¢‘å¼•æ“æ—¶å‡ºé”™: \(error.localizedDescription)")
            throw error
        }
        
        // è·å–éŸ³é¢‘è¾“å…¥æ ¼å¼ï¼ˆå¿…é¡»åœ¨å¼•æ“å¯åŠ¨åï¼‰
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        // éªŒè¯éŸ³é¢‘æ ¼å¼
        guard recordingFormat.sampleRate > 0 && recordingFormat.channelCount > 0 else {
            print("âŒ éŸ³é¢‘æ ¼å¼æ— æ•ˆ: sampleRate=\(recordingFormat.sampleRate), channels=\(recordingFormat.channelCount)")
            audioEngine.stop()
            throw NSError(domain: "SpeechRecognition", code: 4, userInfo: [NSLocalizedDescriptionKey: "éŸ³é¢‘æ ¼å¼æ— æ•ˆ"])
        }
        
        print("âœ… éŸ³é¢‘æ ¼å¼: sampleRate=\(recordingFormat.sampleRate), channels=\(recordingFormat.channelCount)")
        
        // åˆ›å»ºè¯†åˆ«è¯·æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            audioEngine.stop()
            throw NSError(domain: "SpeechRecognition", code: 2, userInfo: [NSLocalizedDescriptionKey: "æ— æ³•åˆ›å»ºè¯†åˆ«è¯·æ±‚"])
        }
        
        recognitionRequest.shouldReportPartialResults = true
        
        // å®‰è£…éŸ³é¢‘ tap
        do {
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
                recognitionRequest.append(buffer)
            }
            print("âœ… Tap å®‰è£…æˆåŠŸ")
        } catch {
            print("âŒ å®‰è£… tap å¤±è´¥: \(error.localizedDescription)")
            audioEngine.stop()
            throw error
        }
        
        // å¼€å§‹è¯†åˆ«ä»»åŠ¡
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            var shouldStop = false
            
            if let result = result {
                let transcription = result.bestTranscription.formattedString
                
                // åªå¤„ç†æ–°å¢çš„éƒ¨åˆ†ï¼ˆé¿å…é‡å¤å¤„ç†ç´¯ç§¯æ–‡æœ¬ï¼‰
                let currentLength = transcription.count
                // æˆåŠŸè¯†åˆ«åˆ°è¯­éŸ³ï¼Œé‡ç½®é”™è¯¯è®¡æ•°
                self.consecutiveNoSpeechErrors = 0
                
                if currentLength > self.lastProcessedLength {
                    let startIndex = transcription.index(transcription.startIndex, offsetBy: self.lastProcessedLength)
                    let newText = String(transcription[startIndex...])
                    print("ğŸ¤ è¯†åˆ«: \(newText)")
                    self.processTranscription(newText)
                    self.lastProcessedLength = currentLength
                }
                
                // æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆç»“æœï¼ˆé™é»˜é‡å¯ï¼Œä¸æ‰“å°æ—¥å¿—ï¼‰
                if result.isFinal {
                    shouldStop = true
                }
            }
            
            if let error = error {
                let errorMessage = error.localizedDescription
                print("âŒ è¯­éŸ³è¯†åˆ«é”™è¯¯: \(errorMessage)")
                
                // æ£€æµ‹ "No speech detected" é”™è¯¯ï¼ˆä»…è®°å½•ï¼Œä¸åœæ­¢ï¼‰
                if errorMessage.contains("No speech detected") {
                    self.consecutiveNoSpeechErrors += 1
                    print("âš ï¸ æ— è¯­éŸ³è¾“å…¥ (\(self.consecutiveNoSpeechErrors) æ¬¡)ï¼Œç»§ç»­ç›‘å¬...")
                    // ä¸åœæ­¢ï¼Œç»§ç»­é‡å¯è¯†åˆ«ä»»åŠ¡
                } else {
                    // å…¶ä»–é”™è¯¯ï¼Œé‡ç½®è®¡æ•°
                    self.consecutiveNoSpeechErrors = 0
                }
                
                shouldStop = true
            }
            
            // å¦‚æœè¯†åˆ«ä»»åŠ¡ç»“æŸï¼Œé™é»˜é‡å¯ï¼ˆä¸åœæ­¢å¼•æ“ï¼‰
            if shouldStop {
                self.recognitionRequest = nil
                self.recognitionTask = nil
                self.lastProcessedLength = 0
                
                // å»¶è¿Ÿ 0.2 ç§’åé‡å¯è¯†åˆ«ä»»åŠ¡ï¼ˆå¼•æ“ä¿æŒè¿è¡Œï¼‰
                DispatchQueue.main.asyncAfter(deadline: .now() + 0.2) {
                    self.restartRecognitionTask()
                }
            }
        }
        
        print("âœ… è¯†åˆ«ä»»åŠ¡å·²å¯åŠ¨")
        
        onStatusChanged?(true)
    }
    
    /// åœæ­¢è¯­éŸ³è¯†åˆ«
    func stopRecognition() {
        print("ğŸ›‘ åœæ­¢è¯­éŸ³è¯†åˆ«")
        
        // åœæ­¢éŸ³é¢‘å¼•æ“
        if audioEngine.isRunning {
            audioEngine.stop()
            print("âœ… éŸ³é¢‘å¼•æ“å·²åœæ­¢")
        }
        
        // ç§»é™¤ tapï¼ˆå¿½ç•¥é”™è¯¯ï¼‰
        do {
            audioEngine.inputNode.removeTap(onBus: 0)
            print("âœ… Tap å·²ç§»é™¤")
        } catch {
            print("âš ï¸ ç§»é™¤ tap æ—¶å‡ºé”™: \(error.localizedDescription)")
        }
        
        // ç»“æŸè¯†åˆ«è¯·æ±‚
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        
        recognitionTask = nil
        recognitionRequest = nil
        lastProcessedLength = 0
        onStatusChanged?(false)
    }
    
    /// å¤„ç†è¯†åˆ«ç»“æœï¼Œä½¿ç”¨æ‹¼éŸ³æ¨¡ç³ŠåŒ¹é…æ–¹å‘å’Œ"ä¸‹ä¸€ä¸ª"æŒ‡ä»¤
    private func processTranscription(_ text: String) {
        // å°†æ–‡æœ¬è½¬æ¢ä¸ºæ‹¼éŸ³
        let pinyin = text.applyingTransform(.mandarinToLatin, reverse: false)?
            .applyingTransform(.stripDiacritics, reverse: false)?
            .lowercased() ?? ""
        
        print("ğŸ”¤ æ‹¼éŸ³: \(pinyin)")
        
        // ä¼˜å…ˆæ£€æŸ¥"ä¸‹ä¸€ä¸ª"æˆ–"ç»§ç»­"æŒ‡ä»¤ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
        // åŒ¹é…ï¼šxia yi ge, ji xu, xia yi, ji
        if pinyin.contains("xia yi") || pinyin.contains("ji xu") || pinyin.contains("jixu") ||
           (pinyin.contains("ji") && !pinyin.contains("shang") && !pinyin.contains("xia") && !pinyin.contains("zuo") && !pinyin.contains("you")) {
            
            // é˜²æ­¢é‡å¤è§¦å‘ï¼š2ç§’å†…åªè§¦å‘ä¸€æ¬¡"ä¸‹ä¸€ä¸ª"
            let now = Date()
            if let lastTime = lastNextCommandTime,
               now.timeIntervalSince(lastTime) < 2.0 {
                print("â­ï¸ å¿½ç•¥é‡å¤çš„ã€Œä¸‹ä¸€ä¸ªã€æŒ‡ä»¤")
                return
            }
            
            lastNextCommandTime = now
            print("â¡ï¸ è¯†åˆ«åˆ°æŒ‡ä»¤: ä¸‹ä¸€ä¸ª")
            onNextCommand?()
            return
        }
        
        // æ¨¡ç³ŠåŒ¹é…æ–¹å‘æ‹¼éŸ³ï¼ˆåªè¦åŒ…å« shang/xia/zuo/you å³å¯ï¼‰
        // ä¼˜å…ˆçº§ï¼šshang > xia > zuo > youï¼ˆé¿å… xia è¯¯åŒ¹é…åˆ° "xia yi ge"ï¼‰
        var recognizedDirection: EDirection?
        
        // æ’é™¤"ä¸‹ä¸€ä¸ª"ä¸­çš„ xia
        let pinyinWithoutNext = pinyin.replacingOccurrences(of: "xia yi", with: "")
        
        if pinyinWithoutNext.contains("shang") {
            recognizedDirection = .up
        } else if pinyinWithoutNext.contains("xia") {
            recognizedDirection = .down
        } else if pinyinWithoutNext.contains("zuo") {
            recognizedDirection = .left
        } else if pinyinWithoutNext.contains("you") || pinyinWithoutNext.contains("yo") {
            recognizedDirection = .right
        }
        
        if let direction = recognizedDirection {
            // é˜²æ­¢é‡å¤è§¦å‘ï¼šå¦‚æœ1ç§’å†…è¯†åˆ«åˆ°ç›¸åŒæ–¹å‘ï¼Œå¿½ç•¥
            let now = Date()
            if let lastDir = lastRecognizedDirection,
               let lastTime = lastRecognizedTime,
               lastDir == direction,
               now.timeIntervalSince(lastTime) < 1.0 {
                print("â­ï¸ å¿½ç•¥é‡å¤è¯†åˆ«: \(direction.name)")
                return
            }
            
            lastRecognizedDirection = direction
            lastRecognizedTime = now
            
            print("âœ… è¯†åˆ«åˆ°æ–¹å‘: \(direction.name) (æ‹¼éŸ³åŒ…å«: \(direction == .up ? "shang" : direction == .down ? "xia" : direction == .left ? "zuo" : "you"))")
            onDirectionRecognized?(direction)
        } else {
            print("âš ï¸ æœªåŒ¹é…åˆ°æ–¹å‘å…³é”®è¯ - åŸæ–‡: \(text), æ‹¼éŸ³: \(pinyin)")
        }
    }
    
    /// æ¸…é™¤è¯†åˆ«å†å²ï¼ˆåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ ¼å­æ—¶è°ƒç”¨ï¼‰
    /// é‡å¯è¯†åˆ«ä»»åŠ¡ï¼ˆä¸é‡å¯å¼•æ“ï¼Œé™é»˜è¿è¡Œï¼‰
    private func restartRecognitionTask() {
        guard audioEngine.isRunning else {
            return
        }
        
        // åˆ›å»ºæ–°çš„è¯†åˆ«è¯·æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            return
        }
        
        recognitionRequest.shouldReportPartialResults = true
        
        let inputNode = audioEngine.inputNode
        
        // å¼€å§‹æ–°çš„è¯†åˆ«ä»»åŠ¡
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            var shouldStop = false
            
            if let result = result {
                let transcription = result.bestTranscription.formattedString
                
                let currentLength = transcription.count
                self.consecutiveNoSpeechErrors = 0
                
                if currentLength > self.lastProcessedLength {
                    let startIndex = transcription.index(transcription.startIndex, offsetBy: self.lastProcessedLength)
                    let newText = String(transcription[startIndex...])
                    print("ğŸ¤ æ–°å¢è¯†åˆ«: \(newText) (å®Œæ•´: \(transcription))")
                    self.processTranscription(newText)
                    self.lastProcessedLength = currentLength
                }
                
                if result.isFinal {
                    shouldStop = true
                }
            }
            
            if let error = error {
                let errorMessage = error.localizedDescription
                
                // åªè®°å½•é"æ— è¯­éŸ³"é”™è¯¯
                if !errorMessage.contains("No speech detected") {
                    print("âŒ è¯­éŸ³è¯†åˆ«é”™è¯¯: \(errorMessage)")
                }
                
                self.consecutiveNoSpeechErrors = 0
                shouldStop = true
            }
            
            if shouldStop {
                self.recognitionRequest = nil
                self.recognitionTask = nil
                self.lastProcessedLength = 0
                
                DispatchQueue.main.asyncAfter(deadline: .now() + 0.2) {
                    self.restartRecognitionTask()
                }
            }
        }
    }
    
    func clearRecognitionHistory() {
        lastRecognizedDirection = nil
        lastRecognizedTime = nil
        lastNextCommandTime = nil
        consecutiveNoSpeechErrors = 0
        
        // ä¸é‡å¯ä»»åŠ¡ï¼Œåªæ ‡è®°å½“å‰ä½ç½®ï¼Œå¿½ç•¥ä¹‹å‰çš„æ‰€æœ‰æ–‡æœ¬
        print("ğŸ§¹ æ¸…é™¤è¯†åˆ«å†å²ï¼ˆæ ‡è®°å¿½ç•¥æ—§æ–‡æœ¬ï¼‰")
        // æ³¨æ„ï¼šä¸é‡ç½®ä¸º 0ï¼Œè€Œæ˜¯ä¿æŒå½“å‰é•¿åº¦ï¼Œè¿™æ ·æ–°æ–‡æœ¬æ‰ä¼šè¢«å¤„ç†
    }
    
    
    /// æ£€æŸ¥æ˜¯å¦å¯ç”¨
    var isAvailable: Bool {
        return speechRecognizer?.isAvailable ?? false
    }
}
